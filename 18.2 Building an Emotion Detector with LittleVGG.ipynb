{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LittleVGG for Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Emotion Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "num_classes = 6\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 16\n",
    "\n",
    "train_data_dir = './fer2013/train'\n",
    "validation_data_dir = './fer2013/validation'\n",
    "\n",
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LittleVGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 48, 48, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 48, 48, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 24, 24, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 24, 24, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 24, 24, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 12, 12, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 12, 12, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 6, 6, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 6, 6, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 3, 3, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                147520    \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1328102 (5.07 MB)\n",
      "Trainable params: 1325926 (5.06 MB)\n",
      "Non-trainable params: 2176 (8.50 KB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/47hy5j7j39379t2ycr9nv4jc0000gn/T/ipykernel_28474/263021586.py:31: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - ETA: 0s - loss: 1.9226 - accuracy: 0.2123\n",
      "Epoch 1: val_loss improved from inf to 1.75973, saving model to /Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5\n",
      "1767/1767 [==============================] - 167s 93ms/step - loss: 1.9226 - accuracy: 0.2123 - val_loss: 1.7597 - val_accuracy: 0.2281 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "   1/1767 [..............................] - ETA: 3:11 - loss: 1.6020 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarvagyasamridhsingh/anaconda3/envs/av/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - ETA: 0s - loss: 1.7472 - accuracy: 0.2487\n",
      "Epoch 2: val_loss improved from 1.75973 to 1.73225, saving model to /Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5\n",
      "1767/1767 [==============================] - 168s 95ms/step - loss: 1.7472 - accuracy: 0.2487 - val_loss: 1.7322 - val_accuracy: 0.2614 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.7381 - accuracy: 0.2545\n",
      "Epoch 3: val_loss did not improve from 1.73225\n",
      "1767/1767 [==============================] - 168s 95ms/step - loss: 1.7381 - accuracy: 0.2545 - val_loss: 1.8125 - val_accuracy: 0.2241 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.7136 - accuracy: 0.2704\n",
      "Epoch 4: val_loss improved from 1.73225 to 1.71017, saving model to /Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5\n",
      "1767/1767 [==============================] - 162s 91ms/step - loss: 1.7136 - accuracy: 0.2704 - val_loss: 1.7102 - val_accuracy: 0.2946 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.6916 - accuracy: 0.2824\n",
      "Epoch 5: val_loss improved from 1.71017 to 1.70198, saving model to /Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5\n",
      "1767/1767 [==============================] - 164s 93ms/step - loss: 1.6916 - accuracy: 0.2824 - val_loss: 1.7020 - val_accuracy: 0.3074 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.6656 - accuracy: 0.2972\n",
      "Epoch 6: val_loss did not improve from 1.70198\n",
      "1767/1767 [==============================] - 171s 97ms/step - loss: 1.6656 - accuracy: 0.2972 - val_loss: 1.7942 - val_accuracy: 0.3131 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.6290 - accuracy: 0.3169\n",
      "Epoch 7: val_loss improved from 1.70198 to 1.54832, saving model to /Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5\n",
      "1767/1767 [==============================] - 175s 99ms/step - loss: 1.6290 - accuracy: 0.3169 - val_loss: 1.5483 - val_accuracy: 0.3994 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.5626 - accuracy: 0.3651\n",
      "Epoch 8: val_loss did not improve from 1.54832\n",
      "1767/1767 [==============================] - 175s 99ms/step - loss: 1.5626 - accuracy: 0.3651 - val_loss: 1.5666 - val_accuracy: 0.4244 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.4966 - accuracy: 0.4005\n",
      "Epoch 9: val_loss improved from 1.54832 to 1.54499, saving model to /Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5\n",
      "1767/1767 [==============================] - 168s 95ms/step - loss: 1.4966 - accuracy: 0.4005 - val_loss: 1.5450 - val_accuracy: 0.4392 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.4637 - accuracy: 0.4189\n",
      "Epoch 10: val_loss improved from 1.54499 to 1.53726, saving model to /Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5\n",
      "1767/1767 [==============================] - 160s 90ms/step - loss: 1.4637 - accuracy: 0.4189 - val_loss: 1.5373 - val_accuracy: 0.4310 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.4252 - accuracy: 0.4360\n",
      "Epoch 11: val_loss improved from 1.53726 to 1.44958, saving model to /Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5\n",
      "1767/1767 [==============================] - 158s 89ms/step - loss: 1.4252 - accuracy: 0.4360 - val_loss: 1.4496 - val_accuracy: 0.4733 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.4108 - accuracy: 0.4422\n",
      "Epoch 12: val_loss did not improve from 1.44958\n",
      "1767/1767 [==============================] - 166s 94ms/step - loss: 1.4108 - accuracy: 0.4422 - val_loss: 1.5014 - val_accuracy: 0.4409 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.3881 - accuracy: 0.4504\n",
      "Epoch 13: val_loss did not improve from 1.44958\n",
      "1767/1767 [==============================] - 167s 94ms/step - loss: 1.3881 - accuracy: 0.4504 - val_loss: 1.4675 - val_accuracy: 0.4648 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "1767/1767 [==============================] - ETA: 0s - loss: 1.3747 - accuracy: 0.4585Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.44958\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "1767/1767 [==============================] - 163s 93ms/step - loss: 1.3747 - accuracy: 0.4585 - val_loss: 1.4531 - val_accuracy: 0.4665 - lr: 0.0010\n",
      "Epoch 14: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"/Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/47hy5j7j39379t2ycr9nv4jc0000gn/T/ipykernel_28474/3546348033.py:23: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[138  18  52  79 165  39]\n",
      " [ 82   8  66 119 126 127]\n",
      " [ 15   9 754  38  44  19]\n",
      " [ 74   9 167 160 130  86]\n",
      " [ 55   8  54 154 302  21]\n",
      " [ 16   6  44  29  11 310]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.36      0.28      0.32       491\n",
      "        Fear       0.14      0.02      0.03       528\n",
      "       Happy       0.66      0.86      0.75       879\n",
      "     Neutral       0.28      0.26      0.27       626\n",
      "         Sad       0.39      0.51      0.44       594\n",
      "    Surprise       0.51      0.75      0.61       416\n",
      "\n",
      "    accuracy                           0.47      3534\n",
      "   macro avg       0.39      0.44      0.40      3534\n",
      "weighted avg       0.41      0.47      0.43      3534\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('/Users/sarvagyasamridhsingh/Documents/Programming/conda3/Objectdetection/udemyCV/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test on some of validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     53\u001b[0m     images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([x])\n\u001b[0;32m---> 54\u001b[0m     classes \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_classes(images, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     55\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(classes)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(files)):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = './fer2013/validation/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    allfaces = []   \n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x,w,y,h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "img = cv2.imread(\"rajeev.jpg\")\n",
    "rects, faces, image = face_detector(img)\n",
    "\n",
    "i = 0\n",
    "for face in faces:\n",
    "    roi = face.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    preds = classifier.predict(roi)[0]\n",
    "    label = class_labels[preds.argmax()]   \n",
    "\n",
    "    #Overlay our detected emotion on our pic\n",
    "    label_position = (rects[i][0] + int((rects[i][1]/2)), abs(rects[i][2] - 10))\n",
    "    i =+ 1\n",
    "    cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Emotion Detector\", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try this on our webcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/var/folders/9b/47hy5j7j39379t2ycr9nv4jc0000gn/T/ipykernel_28474/1099583068.py:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m roi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(roi, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# make a prediction on the ROI, then lookup the class\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m preds \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(roi)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m label \u001b[38;5;241m=\u001b[39m class_labels[preds\u001b[38;5;241m.\u001b[39margmax()]  \n\u001b[1;32m     39\u001b[0m label_position \u001b[38;5;241m=\u001b[39m (rect[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m((rect[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)), rect[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m25\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/av/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/av/lib/python3.11/site-packages/keras/src/engine/training.py:2596\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2588\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2590\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2593\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2594\u001b[0m         )\n\u001b[0;32m-> 2596\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   2597\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m   2598\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2599\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39msteps,\n\u001b[1;32m   2600\u001b[0m     initial_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2601\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   2602\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[1;32m   2603\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[1;32m   2604\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   2605\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2606\u001b[0m     steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   2607\u001b[0m )\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/anaconda3/envs/av/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/av/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1291\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m adapter_cls(\n\u001b[1;32m   1293\u001b[0m     x,\n\u001b[1;32m   1294\u001b[0m     y,\n\u001b[1;32m   1295\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1296\u001b[0m     steps\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[1;32m   1297\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs \u001b[38;5;241m-\u001b[39m initial_epoch,\n\u001b[1;32m   1298\u001b[0m     sample_weights\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1299\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m   1300\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[1;32m   1301\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[1;32m   1302\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   1303\u001b[0m     distribution_strategy\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy(),\n\u001b[1;32m   1304\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1305\u001b[0m     pss_evaluation_shards\u001b[38;5;241m=\u001b[39mpss_evaluation_shards,\n\u001b[1;32m   1306\u001b[0m )\n\u001b[1;32m   1308\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/av/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:314\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# performance.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(permutation)\u001b[38;5;241m.\u001b[39mprefetch(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_batch_indices\u001b[39m(indices):\n\u001b[1;32m    317\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m      A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/av/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2268\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m map_op\u001b[38;5;241m.\u001b[39m_map_v2(\n\u001b[1;32m   2269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2270\u001b[0m     map_func,\n\u001b[1;32m   2271\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mnum_parallel_calls,\n\u001b[1;32m   2272\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[1;32m   2273\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/av/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     35\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/av/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:113\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m structured_function\u001b[38;5;241m.\u001b[39mStructuredFunctionWrapper(\n\u001b[1;32m    108\u001b[0m     map_func,\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformation_name(),\n\u001b[1;32m    110\u001b[0m     dataset\u001b[38;5;241m=\u001b[39minput_dataset,\n\u001b[1;32m    111\u001b[0m     use_legacy_function\u001b[38;5;241m=\u001b[39muse_legacy_function)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m--> 113\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[1;32m    114\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[1;32m    116\u001b[0m     f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    117\u001b[0m     use_inter_op_parallelism\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism,\n\u001b[1;32m    118\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[0;32m~/anaconda3/envs/av/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3471\u001b[0m, in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   3469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3470\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3471\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[1;32m   3472\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMapDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, input_dataset, other_arguments, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, f,\n\u001b[1;32m   3473\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_types\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_types, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shapes\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_shapes,\n\u001b[1;32m   3474\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_inter_op_parallelism\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_inter_op_parallelism,\n\u001b[1;32m   3475\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_cardinality\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_cardinality, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, metadata)\n\u001b[1;32m   3476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3477\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
